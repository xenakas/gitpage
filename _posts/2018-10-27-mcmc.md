---
layout: post
title: "MCMC"
excerpt_separator: "<!--more-->"
categories:
  - Maths
tags:
  - approximation 
  - vocabulary 
  - sampling
  - hmc
  - gibbs
  - nuts
  - metropolis
  - reversible-jump
---


<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


[Markov chain Monte Carlo](https://en.m.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) 
methods comprise a class of algorithms for sampling from a probability distribution.
By constructing a Markov chain that 
has the desired distribution as its equilibrium distribution, one can obtain 
a sample of the desired distribution by observing the chain after a number of steps. 
The more steps there are, the more closely the distribution of the sample matches the actual 
desired distribution.

<!--more-->

Markov chain Monte Carlo methods are primarily used for calculating numerical 
approximations of multi-dimensional integrals, for example in Bayesian statistics

In Bayesian statistics, the recent development of Markov chain Monte Carlo methods has 
been a key step in making it possible to compute large hierarchical models that 
require integrations over hundreds or even thousands of unknown parameters.

In rare event sampling, they are also used for generating 
samples that gradually populate the rare failure region.



#### Software:

R packages: 
- adaptMCMC 
- atmcmc
- BRugs  
- mcmc
- MCMCpack
- ramcmc
- rjags

[PyMC](https://docs.pymc.io)


[Stan](http://mc-stan.org) is a  platform for statistical modeling and high-performance 
statistical computation: full Bayesian statistical inference with MCMC sampling (NUTS, HMC), approximate
Bayesian inference with variational inference (ADVI), penalized maximum likelihood 
estimation with optimization (L-BFGS). 

- Additional R packages provide expression-based linear modeling, 
posterior visualization, and leave-one-out cross-validation. 

- [pystan](https://pystan.readthedocs.io/en/latest/getting_started.html) - Stan package for python



### Convergence

Usually it is not hard to construct a Markov chain with the desired properties. 
The more difficult problem is to determine how many steps are needed to converge to the 
stationary distribution within an acceptable error.

A good chain will have rapid mixing: the stationary distribution is reached quickly 
starting from an arbitrary position. A standard empirical method to assess convergence 
is to run several independent simulated Markov chains and check that the ratio 
of inter-chain to intra-chain variances for all the parameters sampled is close to 1.

Typically, Markov chain Monte Carlo sampling can only approximate the target distribution, 
as there is always some residual effect of the starting position. More sophisticated 
Markov chain Monte Carlo-based algorithms such as coupling from the past can produce 
exact samples, at the cost of additional computation and an unbounded
(though finite in expectation) running time.

Many random walk Monte Carlo methods move around the equilibrium distribution 
in relatively small steps, with no tendency for the steps to proceed in the same direction. 
These methods are easy to implement and analyze, but unfortunately it can take a 
long time for the walker to explore all of the space. The walker will often double
back and cover ground already covered.


### [Effective sample size for MCMC](https://www.johndcook.com/blog/2017/06/27/effective-sample-size-for-mcmc/)


MCMC (Markov Chain Monte Carlo)  lets us draw samples from practically
any probability distribution. But there’s a catch: the samples are not independent. 
This lack of independence means that all the familiar theory on convergence of sums 
of random variables goes out the window.

There’s not much theory to guide assessing the convergence of sums of MCMC samples, 
but there are heuristics. One of these is effective sample size (ESS).
The idea is to have a sort of “exchange rate” between dependent and independent samples. 
You might want to say, for example, that 1,000 samples from a certain Markov chain 
are worth about as much as 80 independent samples because the MCMC samples are highly correlated. 
Or you might want to say that 1,000 samples from a different Markov chain are worth about as
much as 300 independent samples because although the MCMC samples are dependent, 
they’re weakly correlated.

Here’s the definition of ESS:

$$ESS = \dfrac{n}{1 + 2\sum^{\infty}_{k=1} \rho(k) }$$

where $$n$$ is the number of samples and $$\rho(k)$$ is the correlation at lag $$k$$.

This behaves well in the extremes. If your samples are independent, 
your effective samples size equals the actual sample size. 
If the correlation at lag k decreases extremely slowly, so slowly that 
the sum in the denominator diverges, your effective sample size is zero.

Any reasonable Markov chain is between the extremes. 
Zero lag correlation is too much to hope for, but ideally the correlations die 
off fast enough that the sum in the denominator not only converges but also 
isn’t a terribly large value.





## Random walk Monte Carlo methods

When a Markov chain Monte Carlo method is used for approximating a multi-dimensional integral,
an ensemble of "walkers" move around randomly. At each point where a walker steps, 
the integrand value at that point is counted towards the integral. 
The walker then may make a number of tentative steps around the area, 
looking for a place with a reasonably high contribution to the integral to move into next.

Random walk Monte Carlo methods are a kind of random simulation or Monte Carlo method. 
A Markov chain is constructed in such a way as to have the integrand as 
its equilibrium distribution.


### Metropolis–Hastings algorithm 

The Metropolis–Hastings algorithm can draw samples from any probability distribution 
$$P(x)$$, provided that the value of a function $$f(x)$$  proportional to the density 
of $$P$$  can be calculated. 

For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling)
that can directly return independent samples from the distribution, 
and these are free from the problem of autocorrelated samples that is inherent in MCMC methods.

The Metropolis–Hastings algorithm works by generating a sequence of sample values
in such a way that, as more and more sample values are produced, the distribution of values 
more closely approximates the desired distribution $$P(x)$$. 
These sample values are produced iteratively, at each iteration, the algorithm picks 
a candidate for the next sample value 
based on the current sample value.
Then, with some probability, the candidate is either accepted 
(in which case the candidate value is used in the next iteration) 
or rejected (in which case the candidate value is discarded, 
and current value is reused in the next iteration) — 
the probability of acceptance is determined by comparing the values of the function $$f(x)$$  
of the current and candidate sample values with respect to the desired distribution $$P(x)$$


Compared with an algorithm like adaptive rejection sampling that directly generates
independent samples from a distribution, Metropolis–Hastings and other MCMC algorithms 
have a number of disadvantages:

- The samples are correlated. Even though over the long term they do correctly 
follow $$P(x)$$, a set of nearby samples will be correlated with each other 
and not correctly reflect the distribution. 
This means that if we want a set of independent samples, we have to throw away the majority
of samples and only take every $$n$$-th sample, for some value of $$n$$ 
(typically determined by examining the autocorrelation between adjacent samples). 
Autocorrelation can be reduced by increasing the jumping width 
(the average size of a jump, which is related to the variance of the jumping distribution), 
but this will also increase the likelihood of rejection of the proposed jump. 

- Although the Markov chain eventually converges to the desired distribution, 
the initial samples may follow a very different distribution, 
especially if the starting point is in a region of low density. 
As a result, a burn-in period is typically necessary[citation needed], 
where an initial number of samples (e.g. the first 1,000 or so) are thrown away.

On the other hand, most simple **rejection sampling methods suffer** from the 
"curse of dimensionality", where the probability of rejection increases exponentially 
as a function of the number of dimensions. Metropolis–Hastings, along with other MCMC methods, 
do not have this problem to such a degree, and thus are often the only solutions 
available when the number of dimensions of the distribution to be sampled is high. 
As a result, MCMC methods are often the methods of choice for producing samples 
from **hierarchical Bayesian models** and other high-dimensional statistical models used 
nowadays in many disciplines.

In multivariate distributions, the classic Metropolis–Hastings algorithm as
described above involves choosing a new multi-dimensional sample point. 
When the number of dimensions is high, finding the right jumping distribution to use 
can be difficult, as the different individual dimensions behave in very different ways, 
and the jumping width (see above) must be "just right" for all dimensions at once to
avoid excessively slow mixing. An alternative approach that often works better 
in such situations, known as **Gibbs sampling**, involves choosing a new sample for each 
dimension separately from the others, rather than choosing a sample for all dimensions at once. 










### Gibbs sampling


[Gibbs sampling](https://en.m.wikipedia.org/wiki/Gibbs_sampling) is
a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations 
which are approximated from a specified multivariate probability distribution, 
when direct sampling is difficult. 

In its basic version, Gibbs sampling is a special case of the **Metropolis–Hastings algorithm**. 

This method requires all the conditional distributions of the target distribution 
to be sampled exactly.
Gibbs sampling is popular partly because it does not require any 'tuning'.


This sequence can be used to approximate the joint distribution 
(e.g., to generate a histogram of the distribution); to approximate the marginal
distribution of one of the variables, or some subset of the variables 
(for example, the unknown parameters or latent variables);
or to compute an integral (such as the expected value of one of the variables). 


Gibbs sampling is commonly used as a means of statistical inference, especially **Bayesian inference**.
It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), 
and is an alternative to deterministic algorithms for statistical inference 
such as the **expectation-maximization algorithm (EM).**

As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, 
each of which is correlated with nearby samples. As a result, care must be taken if 
independent samples are desired. Generally, samples from the beginning of the chain 
(the burn-in period) may not accurately represent the desired distribution and are usually discarded.



Gibbs sampling is commonly used for **statistical inference** 
(e.g. determining the best value of a parameter). 
The idea is that observed data is incorporated into the sampling process 
by creating separate variables for each piece of observed data and fixing 
the variables in question to their observed values, rather than sampling from those variables. 


The most likely value of a desired parameter (the mode) could then simply be 
selected by choosing the sample value that occurs most commonly; this
is essentially equivalent to **maximum a posteriori estimation** of a parameter. 

More commonly, however, the expected value (mean or average) of the sampled values is chosen;
this is a **Bayes estimator** that takes advantage of the additional data 
about the entire distribution that is available from **Bayesian sampling,**
whereas a maximization algorithm such as expectation maximization (EM) is capable 
of only returning a single point from the distribution. 







### Slice sampling

This method depends on the principle that one can sample from a distribution 
by sampling uniformly from the region under the plot of its density function. 
It alternates uniform sampling in the vertical direction with uniform sampling 
from the horizontal 'slice' defined by the current vertical position.



### Reversible-jump 

In computational statistics, reversible-jump Markov chain Monte Carlo is an extension to standard Markov chain Monte Carlo (MCMC) methodology that allows simulation of the posterior distribution on spaces of varying dimensions.


This method is a variant of the **Metropolis–Hastings algorithm** that allows 
proposals that change the dimensionality of the space.

Reversible-jump variant is useful when doing Markov chain Monte Carlo or Gibbs sampling
over **nonparametric Bayesian models** such as those involving the Dirichlet process or
Chinese restaurant process, where the number of mixing components/clusters/etc. 
is automatically inferred from the data.

There is an experimental RJ-MCMC tool available for the open source BUGs package.


## Non-random walk Markov chain Monte Carlo methods include the following:


### [HMC](https://arxiv.org/pdf/1206.1901.pdf)

Hamiltonian Monte Carlo (originally known as hybrid Monte Carlo) is a Markov chain Monte Carlo (MCMC) 
algorithm that avoids the random walk behavior and sensitivity 
to correlated parameters that plague many MCMC methods by taking a series 
of steps informed by first-order gradient information.


It differs from the Metropolis–Hastings algorithm by reducing the correlation between 
successive sampled states by using a Hamiltonian evolution between states and
additionally by targeting states with a higher acceptance criteria than the observed
probability distribution. This causes it to converge more quickly to the absolute 
probability distribution. 

HMC's performance is highly sensitive to two user-specified parameters: 
a step size and a desired number of steps L. In particular, if L is too small 
then the algorithm exhibits undesirable random walk behavior, 
while if L is too large the algorithm wastes computation. 



### [NUTS](https://arxiv.org/pdf/1111.4246.pdf)

No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number 
of steps L. NUTS uses a recursive algorithm to build a set of likely 
candidate points that spans a wide swath of the target distribution, stopping automatically 
when it starts to double back and retrace its steps. 

Empirically, NUTS perform at least as efficiently as and sometimes more efficiently 
than a well tuned standard HMC method, without requiring user intervention or costly 
tuning runs. We also derive a method for adapting the step size parameter on the 
fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. 
NUTS is also suitable for applications such as BUGS-style automatic inference engines 
that require efficient "turnkey" sampling algorithms.









